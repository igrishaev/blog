---
comment_id: 1751894855121
is_spam: false
is_deleted: false
post: /sql-hate/
date: 2025-07-07 13:27:35 +0000
author_fullname: 'Роман'
---

> Подскажите тогда, как задать лимит в 500.000 документов, пожалуйста.

```json
{
...
"size": 500000,
...
}
```

"can have a significant performance impact" и "нельзя выбрать" вообще не одно и тоже. Can have, а может и не can have - это просто предупреждение. Postgres он тоже не резиновый и имеет свои лимиты. То, что их явно не оговаривают это не значит, что их нет. В частности широко известную проблему с медленным OFFSET я уже упоминал. 

Ну и вы так и не ответили - для каких задач нужна выборка в миллион записей одним запросом? 

> А на какой технологии это была бы одна страница?

It depends ...4 страницы кода на любом ЯП например читать совсем несложно ...а вот сложный SQL с кучей подзапросов и джойнов читается ничуть не лучше безумных json-ов эластика. При этом с определенного количества джойнов понять, что вернется становится очень сложно. Так же как и понять как физически свяжутся данные и как это повлияет на производительность. 

В том случае, что я описывал вся проблема решилась довольно тупо - разбиением этого монстра на 2 отдельных запроса - дальше рыть не стали, ибо проблема ушла. Причем разбивалось именно, чтобы хоть как-то понять, что тут происходит, а не потому, что скилл. 

Даже просто тупо отформатировать достаточно длинный и сильно вложенный SQL до читабельного состояния та еще задача. 

Ну и к слову несмотря на некоторый хейт эластиковые json-ы вполне читаемы. Json сам по себе структурирован, в отличие от SQL-а c местами неочевидными связями. Ну и до кучи - когда-то на волне хайпа трогал монгу. Деталей не помню, но именно язык запросов показался тогда удобным, хотя по памяти там что-то jsono-подобное было.






